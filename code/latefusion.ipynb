{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d88b1f9",
   "metadata": {},
   "source": [
    "# Multimodal Meme Classification (Self-harm Detection)\n",
    "## Late Fusion Approach\n",
    "Notebook ini memisahkan modelling untuk tiap modalitas (Image & Text) sehingga masing-masing model menghasilkan probabilitas klasifikasi sendiri. Di bagian akhir dilakukan *late fusion* (penggabungan prediksi) untuk memperoleh prediksi final yang lebih kuat.\n",
    "\n",
    "**Strategi:**\n",
    "1. Train model gambar (CLIP Vision) -> output probabilitas.\n",
    "2. Train model teks (sentinet/suicidality) -> output probabilitas.\n",
    "3. Evaluasi masing-masing.\n",
    "4. Late Fusion: \n",
    "   - Simple Average \n",
    "   - Weighted Average (grid search bobot terbaik) \n",
    "   - Stacking (Logistic Regression meta-classifier).\n",
    "5. Inferensi akhir dengan bobot terbaik (atau meta-classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4729289a",
   "metadata": {},
   "source": [
    "## 1. Setup & Install (Jika Perlu di Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8182fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch torchvision transformers pandas numpy pillow scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6872eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, warnings\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor, AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed);\n",
    "    torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf4d99",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834224d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "  'csv_path': '/kaggle/input/your-dataset/labels.csv',  # Ubah sesuai dataset\n",
    "  'image_dir': '/kaggle/input/your-dataset/images',      # Ubah sesuai dataset\n",
    "  'num_classes': 2,\n",
    "  'batch_size': 16,\n",
    "  'num_epochs_image': 8,\n",
    "  'num_epochs_text': 8,\n",
    "  'lr_image': 2e-5,\n",
    "  'lr_text': 2e-5,\n",
    "  'weight_decay': 0.01,\n",
    "  'val_split': 0.2,\n",
    "  'early_patience': 4,\n",
    "  'max_text_length': 128,\n",
    "  'checkpoint_dir': '/kaggle/working/checkpoints_late',\n",
    "  'results_dir': '/kaggle/working/results_late',\n",
    "  'random_state': 42\n",
    "}\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "print('CONFIG:')\n",
    "for k,v in CONFIG.items(): print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19eee6",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, clip_processor, text_tokenizer, max_length=128, augment=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.clip_processor = clip_processor\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.label_map = {'Self-harm':1, 'Non Self-harm':0}\n",
    "        self.img_aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.Resize((224,224))\n",
    "        ]) if augment else transforms.Resize((224,224))\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]; img_path = os.path.join(self.image_dir, row['File_Name'])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print('Img error:', img_path, e); image = Image.new('RGB',(224,224),'black')\n",
    "        if self.augment and isinstance(self.img_aug, transforms.Compose):\n",
    "            for t in self.img_aug.transforms: image = t(image)\n",
    "        image_inputs = self.clip_processor(images=image, return_tensors='pt')\n",
    "        image_tensor = image_inputs['pixel_values'].squeeze(0)\n",
    "        text = str(row.get('Teks_Terlihat','')) if pd.notna(row.get('Teks_Terlihat')) else ''\n",
    "        text_inputs = self.text_tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        text_ids = text_inputs['input_ids'].squeeze(0)\n",
    "        text_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "        label = self.label_map.get(row['Label'],0)\n",
    "        return {'image': image_tensor, 'text_ids': text_ids, 'text_mask': text_mask, 'label': torch.tensor(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    df = pd.read_csv(config['csv_path'])\n",
    "    df = df[df['Label'].isin(['Self-harm','Non Self-harm'])].copy()\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, val_df = train_test_split(df, test_size=config['val_split'], stratify=df['Label'], random_state=config['random_state'])\n",
    "    print('Total:', len(df), '| Train:', len(train_df), '| Val:', len(val_df))\n",
    "    clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained('sentinet/suicidality')\n",
    "    train_ds = MemeDataset(train_df, config['image_dir'], clip_processor, text_tokenizer, max_length=config['max_text_length'], augment=True)\n",
    "    val_ds = MemeDataset(val_df, config['image_dir'], clip_processor, text_tokenizer, max_length=config['max_text_length'], augment=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "    val_loader = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "    return train_loader, val_loader, clip_processor, text_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93a332",
   "metadata": {},
   "source": [
    "## 4. Model Image & Text (Terpisah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2, dropout=0.3, freeze=False):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
    "        if freeze:\n",
    "            for p in self.clip.vision_model.parameters(): p.requires_grad=False\n",
    "        dim = self.clip.config.vision_config.hidden_size\n",
    "        self.head = nn.Sequential(nn.Linear(dim,256), nn.ReLU(), nn.Dropout(dropout), nn.Linear(256,num_classes))\n",
    "    def forward(self, pixel_values):\n",
    "        vout = self.clip.vision_model(pixel_values=pixel_values)\n",
    "        emb = vout.pooler_output\n",
    "        return self.head(emb)\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2, dropout=0.3, freeze=False):\n",
    "        super().__init__()\n",
    "        self.text_model = AutoModel.from_pretrained('sentinet/suicidality')\n",
    "        if freeze:\n",
    "            for p in self.text_model.parameters(): p.requires_grad=False\n",
    "        dim = self.text_model.config.hidden_size\n",
    "        self.head = nn.Sequential(nn.Linear(dim,256), nn.ReLU(), nn.Dropout(dropout), nn.Linear(256,num_classes))\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden = out.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).expand(hidden.size()).float()\n",
    "        mean_emb = (hidden*mask).sum(1)/mask.sum(1).clamp(min=1e-9)\n",
    "        return self.head(mean_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a23af",
   "metadata": {},
   "source": [
    "## 5. EarlyStopping & Trainer (Single Modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8daf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=4):\n",
    "        self.patience=patience; self.counter=0; self.best=None; self.stop=False\n",
    "    def step(self, val_loss):\n",
    "        if self.best is None or val_loss < self.best - 1e-4: self.best=val_loss; self.counter=0\n",
    "        else: self.counter+=1; self.stop = self.counter>=self.patience\n",
    "class ModalityTrainer:\n",
    "    def __init__(self, model, loader_train, loader_val, lr, epochs, name):\n",
    "        self.model=model.to(device); self.tr=loader_train; self.val=loader_val; self.lr=lr; self.epochs=epochs; self.name=name\n",
    "        self.crit=nn.CrossEntropyLoss(); self.opt=optim.AdamW(self.model.parameters(), lr=lr, weight_decay=CONFIG['weight_decay'])\n",
    "        self.scheduler=optim.lr_scheduler.ReduceLROnPlateau(self.opt, mode='min', patience=2, factor=0.5)\n",
    "        self.es=EarlyStopping(patience=CONFIG['early_patience'])\n",
    "        self.hist={'train_loss':[], 'val_loss':[], 'val_acc':[]}\n",
    "        self.best_path=os.path.join(CONFIG['checkpoint_dir'], f'{name}_best.pth')\n",
    "    def run_epoch(self):\n",
    "        self.model.train(); tot=0; correct=0; count=0\n",
    "        for b in tqdm(self.tr, desc=f'Train-{self.name}'):\n",
    "            imgs=b['image'].to(device); ids=b['text_ids'].to(device); mask=b['text_mask'].to(device); lbl=b['label'].to(device)\n",
    "            self.opt.zero_grad()\n",
    "            if isinstance(self.model, ImageClassifier): out=self.model(imgs)\n",
    "            else: out=self.model(ids, mask)\n",
    "            loss=self.crit(out,lbl); loss.backward(); torch.nn.utils.clip_grad_norm_(self.model.parameters(),1.0); self.opt.step()\n",
    "            _,pred=out.max(1); correct+= (pred==lbl).sum().item(); count+=lbl.size(0); tot+=loss.item()\n",
    "        return tot/len(self.tr), 100*correct/count\n",
    "    def validate(self):\n",
    "        self.model.eval(); tot=0; correct=0; count=0\n",
    "        with torch.no_grad():\n",
    "            for b in tqdm(self.val, desc=f'Val-{self.name}'):\n",
    "                imgs=b['image'].to(device); ids=b['text_ids'].to(device); mask=b['text_mask'].to(device); lbl=b['label'].to(device)\n",
    "                if isinstance(self.model, ImageClassifier): out=self.model(imgs)\n",
    "                else: out=self.model(ids, mask)\n",
    "                loss=self.crit(out,lbl); _,pred=out.max(1); correct+= (pred==lbl).sum().item(); count+=lbl.size(0); tot+=loss.item()\n",
    "        return tot/len(self.val), 100*correct/count\n",
    "    def train(self):\n",
    "        for ep in range(1, self.epochs+1):\n",
    "            tl,ta=self.run_epoch(); vl,va=self.validate(); self.scheduler.step(vl)\n",
    "            self.hist['train_loss'].append(tl); self.hist['val_loss'].append(vl); self.hist['val_acc'].append(va)\n",
    "            print(f'Epoch {ep}/{self.epochs} | TrainLoss {tl:.4f} | ValLoss {vl:.4f} | ValAcc {va:.2f}%')\n",
    "            if self.es.best is None or vl < self.es.best: torch.save(self.model.state_dict(), self.best_path)\n",
    "            self.es.step(vl);\n",
    "            if self.es.stop: print('Early stop'); break\n",
    "        return self.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b4575",
   "metadata": {},
   "source": [
    "## 6. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7efa77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, clip_processor_ref, text_tokenizer_ref = load_data(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd97fa4",
   "metadata": {},
   "source": [
    "## 7. Train Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = ImageClassifier(num_classes=CONFIG['num_classes'])\n",
    "trainer_img = ModalityTrainer(image_model, train_loader, val_loader, CONFIG['lr_image'], CONFIG['num_epochs_image'], 'image')\n",
    "hist_img = trainer_img.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e4184",
   "metadata": {},
   "source": [
    "## 8. Train Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59551ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = TextClassifier(num_classes=CONFIG['num_classes'])\n",
    "trainer_txt = ModalityTrainer(text_model, train_loader, val_loader, CONFIG['lr_text'], CONFIG['num_epochs_text'], 'text')\n",
    "hist_txt = trainer_txt.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915be3df",
   "metadata": {},
   "source": [
    "## 9. Evaluation Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_probabilities(model, loader):\n",
    "    model.eval(); all_probs=[]; all_labels=[];\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            imgs=b['image'].to(device); ids=b['text_ids'].to(device); mask=b['text_mask'].to(device); lbl=b['label'].to(device)\n",
    "            if isinstance(model, ImageClassifier): out=model(imgs)\n",
    "            else: out=model(ids, mask)\n",
    "            probs=torch.softmax(out, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy()); all_labels.append(lbl.cpu().numpy())\n",
    "    all_probs=np.concatenate(all_probs); all_labels=np.concatenate(all_labels)\n",
    "    preds = all_probs.argmax(1)\n",
    "    acc=accuracy_score(all_labels,preds); prec=precision_score(all_labels,preds,average='weighted',zero_division=0); rec=recall_score(all_labels,preds,average='weighted',zero_division=0); f1=f1_score(all_labels,preds,average='weighted',zero_division=0)\n",
    "    return {'probs':all_probs,'labels':all_labels,'metrics':{'acc':acc,'prec':prec,'rec':rec,'f1':f1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28806da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights (if early stopping) before evaluation\n",
    "img_best = os.path.join(CONFIG['checkpoint_dir'],'image_best.pth')\n",
    "txt_best = os.path.join(CONFIG['checkpoint_dir'],'text_best.pth')\n",
    "if os.path.exists(img_best): image_model.load_state_dict(torch.load(img_best, map_location=device))\n",
    "if os.path.exists(txt_best): text_model.load_state_dict(torch.load(txt_best, map_location=device))\n",
    "eval_img = evaluate_probabilities(image_model, val_loader)\n",
    "eval_txt = evaluate_probabilities(text_model, val_loader)\n",
    "print('Image Metrics:', eval_img['metrics'])\n",
    "print('Text Metrics:', eval_txt['metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050ab87",
   "metadata": {},
   "source": [
    "## 10. Late Fusion (Simple, Weighted, Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = eval_img['labels']  # sama dengan eval_txt['labels']\n",
    "img_probs = eval_img['probs']\n",
    "txt_probs = eval_txt['probs']\n",
    "# Simple Average\n",
    "simple_probs = (img_probs + txt_probs)/2.0\n",
    "simple_preds = simple_probs.argmax(1)\n",
    "def metrics_from(preds, labels, probs):\n",
    "    return {\n",
    "        'acc': accuracy_score(labels,preds),\n",
    "        'prec': precision_score(labels,preds,average='weighted',zero_division=0),\n",
    "        'rec': recall_score(labels,preds,average='weighted',zero_division=0),\n",
    "        'f1': f1_score(labels,preds,average='weighted',zero_division=0)\n",
    "    }\n",
    "simple_metrics = metrics_from(simple_preds, labels, simple_probs)\n",
    "print('Simple Average Metrics:', simple_metrics)\n",
    "# Weighted Average Grid Search\n",
    "best_w = None; best_f1=-1; best_metrics=None; best_probs=None\n",
    "for w in np.linspace(0,1,21):  # step 0.05\n",
    "    fused = w*img_probs + (1-w)*txt_probs\n",
    "    preds = fused.argmax(1)\n",
    "    m = metrics_from(preds, labels, fused)\n",
    "    if m['f1'] > best_f1: best_f1=m['f1']; best_w=w; best_metrics=m; best_probs=fused\n",
    "print(f'Best Weight (img vs text): {best_w:.2f} | F1={best_f1:.4f}')\n",
    "print('Weighted Metrics:', best_metrics)\n",
    "# Stacking (Logistic Regression) menggunakan probabilitas kedua model sebagai fitur\n",
    "stack_features = np.concatenate([img_probs, txt_probs], axis=1)  # shape (N,4)\n",
    "meta = LogisticRegression(max_iter=1000)\n",
    "meta.fit(stack_features, labels)\n",
    "stack_preds = meta.predict(stack_features)\n",
    "stack_metrics = metrics_from(stack_preds, labels, None)\n",
    "print('Stacking Metrics:', stack_metrics)\n",
    "# Tentukan pendekatan terbaik berdasarkan F1\n",
    "fusion_results = {\n",
    "  'simple': simple_metrics,\n",
    "  'weighted': best_metrics,\n",
    "  'stacking': stack_metrics,\n",
    "  'best_weight': best_w\n",
    "}\n",
    "pd.DataFrame(fusion_results).to_csv(os.path.join(CONFIG['results_dir'],'fusion_metrics.csv'))\n",
    "print('âœ“ Fusion metrics disimpan.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5e1ce",
   "metadata": {},
   "source": [
    "## 11. Visualisasi Training Curves (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd558d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist, title):\n",
    "    epochs = range(1, len(hist['train_loss'])+1)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1); plt.plot(epochs, hist['train_loss'], label='Train'); plt.plot(epochs, hist['val_loss'], label='Val'); plt.title(title+' Loss'); plt.legend(); plt.grid(alpha=0.3)\n",
    "    plt.subplot(1,2,2); plt.plot(epochs, hist['val_acc'], label='Val Acc', color='green'); plt.title(title+' Val Accuracy'); plt.legend(); plt.grid(alpha=0.3)\n",
    "    plt.tight_layout(); plt.show()\n",
    "plot_history(hist_img, 'Image'); plot_history(hist_txt, 'Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb9fde",
   "metadata": {},
   "source": [
    "## 12. Inference (Late Fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27547d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_late_fusion(image_model, text_model, image_path, text, clip_processor, text_tokenizer, weight=None, meta_clf=None):\n",
    "    image_model.eval(); text_model.eval()\n",
    "    # Image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_inputs = clip_processor(images=img, return_tensors='pt')['pixel_values'].to(device)\n",
    "    # Text\n",
    "    text_inputs = text_tokenizer(str(text), padding='max_length', truncation=True, max_length=CONFIG['max_text_length'], return_tensors='pt')\n",
    "    ids = text_inputs['input_ids'].to(device); mask = text_inputs['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        img_logits = image_model(img_inputs)\n",
    "        txt_logits = text_model(ids, mask)\n",
    "        img_p = torch.softmax(img_logits, dim=1).cpu().numpy()[0]\n",
    "        txt_p = torch.softmax(txt_logits, dim=1).cpu().numpy()[0]\n",
    "    if meta_clf is not None:\n",
    "        feat = np.concatenate([img_p, txt_p])[None,:]\n",
    "        pred = meta_clf.predict(feat)[0]\n",
    "        conf = (img_p[1]+txt_p[1])/2\n",
    "        fused = None\n",
    "    else:\n",
    "        w = 0.5 if weight is None else weight\n",
    "        fused = w*img_p + (1-w)*txt_p\n",
    "        pred = fused.argmax(); conf = fused[pred]\n",
    "    label_map = ['Non Self-harm','Self-harm']\n",
    "    return {\n",
    "        'final_label': label_map[pred],\n",
    "        'confidence': float(conf),\n",
    "        'image_probs': {label_map[0]: float(img_p[0]), label_map[1]: float(img_p[1])},\n",
    "        'text_probs': {label_map[0]: float(txt_p[0]), label_map[1]: float(txt_p[1])},\n",
    "        'fused_probs': None if fused is None else {label_map[0]: float(fused[0]), label_map[1]: float(fused[1])},\n",
    "        'weight_used': None if meta_clf is not None else (0.5 if weight is None else weight),\n",
    "        'stacking_used': meta_clf is not None\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7dc477",
   "metadata": {},
   "source": [
    "## 13. Ringkasan Akhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a177d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('LATE FUSION PIPELINE SELESAI')\n",
    "print('Device:', device)\n",
    "print('Image Best Path:', img_best)\n",
    "print('Text Best Path:', txt_best)\n",
    "print('Best Weighted w:', fusion_results['best_weight'])\n",
    "print('Metrics:')\n",
    "for k,v in fusion_results.items():\n",
    "    if k in ['simple','weighted','stacking']:\n",
    "        print(f'  {k}: F1={v[\"f1\"]:.4f}, Acc={v[\"acc\"]:.4f}')\n",
    "print('Fusion metrics saved to fusion_metrics.csv')\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
